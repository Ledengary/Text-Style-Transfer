{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import codecs\n",
    "from os import listdir\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils.vis_utils import plot_model\n",
    "language = 'ENGLISH'\n",
    "path_prefix = ''\n",
    "if language == 'PERSIAN':\n",
    "    path_prefix = 'English files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(given_path):\n",
    "    docs = []\n",
    "    counter = 0\n",
    "    for fileName in listdir(given_path):\n",
    "        counter += 1\n",
    "        file = open(given_path + fileName, 'r', encoding='utf-8')\n",
    "        text = file.read()\n",
    "        docs.append(text.split())\n",
    "        file.close()\n",
    "        if language == 'ENGLISH' and counter == 50000: break\n",
    "    return docs\n",
    "\n",
    "formal_male_data = None\n",
    "formal_female_data = None\n",
    "if language == 'PERSIAN':\n",
    "    formal_male_data = load_docs('Gender tagged corpus cleaned/NEW/formal language/male/')\n",
    "    formal_female_data = load_docs('Gender tagged corpus cleaned/NEW/formal language/female/')\n",
    "    print('PERSIAN docs loaded')\n",
    "else:\n",
    "    formal_male_data = load_docs('Gender tagged corpus English/male/')\n",
    "    formal_female_data = load_docs('Gender tagged corpus English/female/')\n",
    "    print('ENGLISH docs loaded')\n",
    "\n",
    "all_data = formal_male_data + formal_female_data\n",
    "all_labels = [1] * len(formal_male_data) + [0] * len(formal_female_data)\n",
    "print(len(all_data))\n",
    "print(len(formal_male_data), len(formal_female_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = None\n",
    "if language == 'PERSIAN':\n",
    "    file = open('Persian Word Embedding/cc.fa.300.vec', 'r', encoding='utf-8', errors='ignore')\n",
    "else:\n",
    "    file = open('English Word Embedding/cc.en.300.vec', 'r', encoding='utf-8', errors='ignore')\n",
    "\n",
    "vocab_and_vectors = {}\n",
    "\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0].encode('utf-8').decode('utf-8')\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    vocab_and_vectors[word] = vector\n",
    "\n",
    "print(len(vocab_and_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "  \n",
    "# choose some words to be stemmed \n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 300\n",
    "tokenizer = Tokenizer(num_words = features)\n",
    "tokenizer.fit_on_texts(all_data)\n",
    "with open(path_prefix + 'GenderTokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Tokenizer saved.\")\n",
    "word_index = tokenizer.word_index\n",
    "X = tokenizer.texts_to_sequences(all_data)\n",
    "max_length = max([len(sent) for sent in X])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "X = pad_sequences(X, padding='post')\n",
    "y = all_labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)\n",
    "print(len(X_train), len(X_val), len(X_test))\n",
    "print(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = vocab_and_vectors.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def define_model(vocab_size, length):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    LSTM1 = LSTM(256, return_sequences=True, recurrent_dropout=0.2)(pool1)\n",
    "    flat1 = Flatten()(LSTM1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    LSTM2 = LSTM(256, return_sequences=True, recurrent_dropout=0.2)(pool2)\n",
    "    flat2 = Flatten()(LSTM2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    LSTM3 = LSTM(256, return_sequences=True, recurrent_dropout=0.2)(pool3)\n",
    "    flat3 = Flatten()(LSTM3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu' )(merged)\n",
    "    outputs = Dense(1, activation='sigmoid' )(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    model. compile(loss='binary_crossentropy' , optimizer='adam' , metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "checkpoint = ModelCheckpoint(path_prefix + \"model.h5\", monitor='val_acc', save_best_only=True, mode='max')\n",
    "model.fit([X_train, X_train, X_train], y_train, epochs=10, validation_data = ([X_val, X_val, X_val], y_val), batch_size=256, callbacks=[checkpoint])\n",
    "if language == 'PERSIAN':\n",
    "    model.save('saved models/Keras/model.h5' )\n",
    "else:\n",
    "    model.save(path_prefix + 'modelMain.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "if language == 'PERSIAN':\n",
    "    model = load_model('saved models/Keras/model.h5' )\n",
    "else:\n",
    "    model = load_model(path_prefix + 'modelMain.h5' )\n",
    "    \n",
    "_, acc = model.evaluate([X_train, X_train, X_train], y_train)\n",
    "print('Train Accuracy: %.2f' % (acc*100))\n",
    "d, acc = model.evaluate([X_test, X_test, X_test], y_test)\n",
    "print('Test Accuracy: %.2f' % (acc*100), d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats = model.predict([X_test, X_test, X_test])\n",
    "right_or_wrong = []\n",
    "for i in range(0, len(X_test)):\n",
    "    prediction = 0 if yhats[i] < 0.5 else 1\n",
    "    if prediction == y_test[i]:\n",
    "        right_or_wrong.append('RIGHT')\n",
    "    else:\n",
    "        right_or_wrong.append('WRONG')\n",
    "        \n",
    "print(len(right_or_wrong), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_seq(sequence):\n",
    "    main_sentence = []\n",
    "    for tokenNumber in sequence:\n",
    "        if tokenNumber != 0: \n",
    "            tokenWord = list(word_index.keys())[list(word_index.values()).index(tokenNumber)]\n",
    "            main_sentence.append(tokenWord)\n",
    "    return main_sentence\n",
    "    \n",
    "print(X_test[0])\n",
    "print(index_to_seq(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "file = codecs.open(path_prefix + 'Training.txt', 'w', \"utf-8\")\n",
    "counter = 0\n",
    "for eachDoc in X_train:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    for eachToken in index_to_seq(eachDoc):\n",
    "        file.write(eachToken + '\\n')\n",
    "    file.write(\"====\\n\")\n",
    "file.close()\n",
    "print(\"1\")\n",
    "counter = 0\n",
    "file = codecs.open(path_prefix + 'TrainingAnswers.txt', 'w', \"utf-8\")\n",
    "for eachAnswer in y_train:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    file.write(str(eachAnswer) + '\\n')\n",
    "file.close()\n",
    "print(\"2\")\n",
    "file = codecs.open(path_prefix + 'Validing.txt', 'w', \"utf-8\")\n",
    "counter = 0\n",
    "for eachDoc in X_val:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    for eachToken in index_to_seq(eachDoc):\n",
    "        file.write(eachToken + '\\n')\n",
    "    file.write(\"====\\n\")\n",
    "file.close()\n",
    "print(\"3\")\n",
    "counter = 0\n",
    "file = codecs.open(path_prefix + 'ValidingAnswers.txt', 'w', \"utf-8\")\n",
    "for eachAnswer in y_val:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    file.write(str(eachAnswer) + '\\n')\n",
    "file.close()\n",
    "print(\"4\")\n",
    "file = codecs.open(path_prefix + 'Testing.txt', 'w', \"utf-8\")\n",
    "counter = 0\n",
    "for eachDoc in X_test:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    for eachToken in index_to_seq(eachDoc):\n",
    "        file.write(eachToken + '\\n')\n",
    "    file.write(\"====\\n\")\n",
    "file.close()\n",
    "print(\"5\")\n",
    "counter = 0\n",
    "file = codecs.open(path_prefix + 'TestingAnswers.txt', 'w', \"utf-8\")\n",
    "for eachAnswer in y_test:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    file.write(str(eachAnswer) + '\\n')\n",
    "file.close()\n",
    "print(\"6\")\n",
    "counter = 0\n",
    "file = codecs.open(path_prefix + 'TestingSanity.txt', 'w', \"utf-8\")\n",
    "for eachAnswer in right_or_wrong:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    file.write(str(eachAnswer) + '\\n')\n",
    "file.close()\n",
    "print(\"7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_doc(text, vocab):\n",
    "    tokens = text.split()\n",
    "    re_punc = re. compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub(' ', w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return tokens\n",
    "    \n",
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    line = clear_doc(review, vocab)\n",
    "    # encode and pad review\n",
    "    padded = pad_sequences(X, padding='post')\n",
    "    # predict sentiment\n",
    "    print(\"Predicting...\")\n",
    "    yhat = model.predict([padded, padded, padded])\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return percent_pos, 'FEMALE'\n",
    "    return percent_pos, 'MALE'\n",
    "\n",
    "text = input('Enter Test Case: ')\n",
    "vocab = list(vocab_and_vectors.keys())\n",
    "percent, prediction = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print(percent, prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
